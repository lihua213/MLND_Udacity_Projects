{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Doesn't run through kaggle scripts because of the 20 min limit. \n",
    "Download the code, expand the network, add more iterations\n",
    "'''\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "import lasagne\n",
    "from lasagne.layers import helper\n",
    "from lasagne.updates import adam\n",
    "from lasagne.nonlinearities import rectify, softmax\n",
    "from lasagne.layers import InputLayer, MaxPool2DLayer, DenseLayer, DropoutLayer, helper\n",
    "from lasagne.layers import Conv2DLayer as ConvLayer\n",
    "\n",
    "import theano\n",
    "from theano import tensor as T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read train images\n",
      "Load folder c0\n",
      "../input/train/c0/*.jpg\n",
      "Load folder c1\n",
      "../input/train/c1/*.jpg\n",
      "Load folder c2\n",
      "../input/train/c2/*.jpg\n",
      "Load folder c3\n",
      "../input/train/c3/*.jpg\n",
      "Load folder c4\n",
      "../input/train/c4/*.jpg\n",
      "Load folder c5\n",
      "../input/train/c5/*.jpg\n",
      "Load folder c6\n",
      "../input/train/c6/*.jpg\n",
      "Load folder c7\n",
      "../input/train/c7/*.jpg\n",
      "Load folder c8\n",
      "../input/train/c8/*.jpg\n",
      "Load folder c9\n",
      "../input/train/c9/*.jpg\n",
      "('Train shape:', (0, 1, 24, 24), 'Test shape:', (0, 1, 24, 24))\n",
      "Read test images\n",
      "('iter:', 0, '| TL:', nan, '| VL:', nan, '| Vacc:', nan, '| Ratio:', nan, '| Time:', 0.0)\n",
      "('iter:', 1, '| TL:', nan, '| VL:', nan, '| Vacc:', nan, '| Ratio:', nan, '| Time:', 0.0)\n",
      "Making predictions\n",
      "pred shape\n",
      "(0,)\n",
      "Creating Submission\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Empty data passed with indices specified.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-812d9799b4bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0mresult1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'submission_ZFTurboNet.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m \u001b[0mcreate_submission\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-812d9799b4bd>\u001b[0m in \u001b[0;36mcreate_submission\u001b[0;34m(predictions, test_id)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Creating Submission'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_submission\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m     \u001b[0mresult1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'c0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'c1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'c2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'c3'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'c4'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'c5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'c6'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'c7'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'c8'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'c9'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m     \u001b[0mresult1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'img'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresult1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0mresult1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'submission_ZFTurboNet.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 mgr = self._init_ndarray(data, index, columns, dtype=dtype,\n\u001b[0;32m--> 255\u001b[0;31m                                          copy=copy)\n\u001b[0m\u001b[1;32m    256\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_init_ndarray\u001b[0;34m(self, values, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_possibly_infer_to_datetimelike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcreate_block_manager_from_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mcreate_block_manager_from_blocks\u001b[0;34m(blocks, axes)\u001b[0m\n\u001b[1;32m   3991\u001b[0m         \u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'values'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3992\u001b[0m         \u001b[0mtot_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3993\u001b[0;31m         \u001b[0mconstruction_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtot_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3994\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3995\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mconstruction_error\u001b[0;34m(tot_items, block_shape, axes, e)\u001b[0m\n\u001b[1;32m   3966\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3967\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mblock_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3968\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Empty data passed with indices specified.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3969\u001b[0m     raise ValueError(\"Shape of passed values is {0}, indices imply {1}\".format(\n\u001b[1;32m   3970\u001b[0m         passed, implied))\n",
      "\u001b[0;31mValueError\u001b[0m: Empty data passed with indices specified."
     ]
    }
   ],
   "source": [
    "'''\n",
    "Loading data functions\n",
    "'''\n",
    "PIXELS = 24\n",
    "imageSize = PIXELS * PIXELS\n",
    "num_features = imageSize \n",
    "\n",
    "def load_train_cv(encoder):\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    print('Read train images')\n",
    "    for j in range(10):\n",
    "        print('Load folder c{}'.format(j))\n",
    "        path = os.path.join('..', 'input', 'train', 'c' + str(j), '*.jpg')\n",
    "        print path\n",
    "        files = glob.glob(path)\n",
    "        for fl in files:\n",
    "            img = cv2.imread(fl,0)\n",
    "            img = cv2.resize(img, (PIXELS, PIXELS))\n",
    "            #img = img.transpose(2, 0, 1)\n",
    "            img = np.reshape(img, (1, num_features))\n",
    "            X_train.append(img)\n",
    "            y_train.append(j)\n",
    "\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "\n",
    "    y_train = encoder.fit_transform(y_train).astype('int32')\n",
    "\n",
    "    X_train, y_train = shuffle(X_train, y_train)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.1)\n",
    "\n",
    "    X_train = X_train.reshape(X_train.shape[0], 1, PIXELS, PIXELS).astype('float32') / 255.\n",
    "    X_test = X_test.reshape(X_test.shape[0], 1, PIXELS, PIXELS).astype('float32') / 255.\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, encoder\n",
    "\n",
    "def load_test():\n",
    "    print('Read test images')\n",
    "    path = os.path.join('..', 'input', 'test', '*.jpg')\n",
    "    files = glob.glob(path)\n",
    "    X_test = []\n",
    "    X_test_id = []\n",
    "    total = 0\n",
    "    thr = math.floor(len(files)/10)\n",
    "    for fl in files:\n",
    "        flbase = os.path.basename(fl)\n",
    "        img = cv2.imread(fl,0)\n",
    "        img = cv2.resize(img, (PIXELS, PIXELS))\n",
    "        #img = img.transpose(2, 0, 1)\n",
    "        img = np.reshape(img, (1, num_features))\n",
    "        X_test.append(img)\n",
    "        X_test_id.append(flbase)\n",
    "        total += 1\n",
    "        if total%thr == 0:\n",
    "            print('Read {} images from {}'.format(total, len(files)))\n",
    "\n",
    "    X_test = np.array(X_test)\n",
    "    X_test_id = np.array(X_test_id)\n",
    "\n",
    "    X_test = X_test.reshape(X_test.shape[0], 1, PIXELS, PIXELS).astype('float32') / 255.\n",
    "\n",
    "    return X_test, X_test_id\n",
    "\n",
    "\n",
    "'''\n",
    "Lasagne Model ZFTurboNet and Batch Iterator\n",
    "'''\n",
    "def ZFTurboNet(input_var=None):\n",
    "    l_in = InputLayer(shape=(None, 1, PIXELS, PIXELS), input_var=input_var)\n",
    "\n",
    "    l_conv = ConvLayer(l_in, num_filters=8, filter_size=3, pad=1, nonlinearity=rectify)\n",
    "    l_convb = ConvLayer(l_conv, num_filters=8, filter_size=3, pad=1, nonlinearity=rectify)\n",
    "    l_pool = MaxPool2DLayer(l_convb, pool_size=2) # feature maps 12x12\n",
    "\n",
    "    #l_dropout1 = DropoutLayer(l_pool, p=0.25)\n",
    "    l_hidden = DenseLayer(l_pool, num_units=128, nonlinearity=rectify)\n",
    "    #l_dropout2 = DropoutLayer(l_hidden, p=0.5)\n",
    "\n",
    "    l_out = DenseLayer(l_hidden, num_units=10, nonlinearity=softmax)\n",
    "\n",
    "    return l_out\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize):\n",
    "    assert len(inputs) == len(targets)\n",
    "    indices = np.arange(len(inputs))\n",
    "    np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        yield inputs[excerpt], targets[excerpt]\n",
    "\n",
    "\"\"\"\n",
    "Set up all theano functions\n",
    "\"\"\"\n",
    "BATCHSIZE = 32\n",
    "LR = 0.001\n",
    "ITERS = 2\n",
    "\n",
    "X = T.tensor4('X')\n",
    "Y = T.ivector('y')\n",
    "\n",
    "# set up theano functions to generate output by feeding data through network, any test outputs should be deterministic\n",
    "output_layer = ZFTurboNet(X)\n",
    "output_train = lasagne.layers.get_output(output_layer)\n",
    "output_test = lasagne.layers.get_output(output_layer, deterministic=True)\n",
    "\n",
    "# set up the loss that we aim to minimize, when using cat cross entropy our Y should be ints not one-hot\n",
    "loss = lasagne.objectives.categorical_crossentropy(output_train, Y)\n",
    "loss = loss.mean()\n",
    "\n",
    "# set up loss functions for validation dataset\n",
    "valid_loss = lasagne.objectives.categorical_crossentropy(output_test, Y)\n",
    "valid_loss = valid_loss.mean()\n",
    "\n",
    "valid_acc = T.mean(T.eq(T.argmax(output_test, axis=1), Y), dtype=theano.config.floatX)\n",
    "\n",
    "# get parameters from network and set up sgd with nesterov momentum to update parameters\n",
    "params = lasagne.layers.get_all_params(output_layer, trainable=True)\n",
    "updates = adam(loss, params, learning_rate=LR)\n",
    "\n",
    "# set up training and prediction functions\n",
    "train_fn = theano.function(inputs=[X,Y], outputs=loss, updates=updates)\n",
    "valid_fn = theano.function(inputs=[X,Y], outputs=[valid_loss, valid_acc])\n",
    "\n",
    "# set up prediction function\n",
    "predict_proba = theano.function(inputs=[X], outputs=output_test)\n",
    "\n",
    "'''\n",
    "load training data and start training\n",
    "'''\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# load the training and validation data sets\n",
    "train_X, train_y, valid_X, valid_y, encoder = load_train_cv(encoder)\n",
    "print('Train shape:', train_X.shape, 'Test shape:', valid_X.shape)\n",
    "\n",
    "# load data\n",
    "X_test, X_test_id = load_test()\n",
    "\n",
    "# loop over training functions for however many iterations, print information while training\n",
    "try:\n",
    "    for epoch in range(ITERS):\n",
    "        # do the training\n",
    "        start = time.time()\n",
    "        # training batches\n",
    "        train_loss = []\n",
    "        for batch in iterate_minibatches(train_X, train_y, BATCHSIZE):\n",
    "            inputs, targets = batch\n",
    "            train_loss.append(train_fn(inputs, targets))\n",
    "        train_loss = np.mean(train_loss)\n",
    "        # validation batches\n",
    "        valid_loss = []\n",
    "        valid_acc = []\n",
    "        for batch in iterate_minibatches(valid_X, valid_y, BATCHSIZE):\n",
    "            inputs, targets = batch\n",
    "            valid_eval = valid_fn(inputs, targets)\n",
    "            valid_loss.append(valid_eval[0])\n",
    "            valid_acc.append(valid_eval[1])\n",
    "        valid_loss = np.mean(valid_loss)\n",
    "        valid_acc = np.mean(valid_acc)\n",
    "        # get ratio of TL to VL\n",
    "        ratio = train_loss / valid_loss\n",
    "        end = time.time() - start\n",
    "        # print training details\n",
    "        print('iter:', epoch, '| TL:', np.round(train_loss,decimals=3), '| VL:', np.round(valid_loss, decimals=3), '| Vacc:', np.round(valid_acc, decimals=3), '| Ratio:', np.round(ratio, decimals=2), '| Time:', np.round(end, decimals=1))\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "\n",
    "'''\n",
    "Make Submission\n",
    "'''\n",
    "\n",
    "#make predictions\n",
    "print('Making predictions')\n",
    "PRED_BATCH = 2\n",
    "def iterate_pred_minibatches(inputs, batchsize):\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt]\n",
    "\n",
    "predictions = []\n",
    "for pred_batch in iterate_pred_minibatches(X_test, PRED_BATCH):\n",
    "    predictions.extend(predict_proba(pred_batch))\n",
    "\n",
    "predictions = np.array(predictions)\n",
    "\n",
    "print('pred shape')\n",
    "print(predictions.shape)\n",
    "\n",
    "print('Creating Submission')\n",
    "def create_submission(predictions, test_id):\n",
    "    result1 = pd.DataFrame(predictions, columns=['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9'])\n",
    "    result1.loc[:, 'img'] = pd.Series(test_id, index=result1.index)\n",
    "    result1.to_csv('submission_ZFTurboNet.csv', index=False)\n",
    "\n",
    "create_submission(predictions, X_test_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
