{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "#### Project Report\n",
    "\n",
    "\n",
    "\n",
    "#### Implementing a Basic Driving Agent: Random driving. \n",
    "\n",
    "The first task was to implement a basic driving agent. For this part, I implemented a random sequence generator to choose from an action from a given set of actions. The random action selection was implemented by generating a random integer between 0 and 3, and using this as index to identify the next set of action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Select action according to your policy\n",
    "action_random = [None, 'forward', 'left', 'right']\n",
    "rand_action = action_random[random.randint(0,3)]\n",
    "action = rand_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question: \n",
    "Observe what you see with the agent's behavior as it takes random actions. Does the smartcab eventually make it to the destination? Are there any other interesting observations to note?\n",
    "\n",
    "#### Response: \n",
    "\n",
    "To test the behavior of the random agent, I ran simulation with no deadline enforced. A stopping criteria was implemented where if the car did not reach the desired target in 100 more than allowed steps, the simulation ended. In about 75% of the trials, the car did not reach the target. The red dots indicate the trials where the simulation was terminated because the number of steps were 100 more than the allowed time. Simulation was terminated due to long run time in 25% of the cases. The blue line indicates deadline, therefore the trip ended early if the black dot was above the blue line.\n",
    "\n",
    "<img src=\"random_state.png\">\n",
    "\n",
    "The random agent completely disregards the location of the target and the neighboring cars while choosing its actions. However, as the car is exploring the entire area, it is expected that it will visit all the states. \n",
    "\n",
    "Note: To get the plots above, I modified the environment.py function to return steps taken and desired time (deadline). The code to perform random actions is presented in agent_random.py."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question: \n",
    "What states have you identified that are appropriate for modeling the smartcab and environment? Why do you believe each of these states to be appropriate for this problem?\n",
    "\n",
    "#### Response: \n",
    "\n",
    "States of the system were chosen as follows, \n",
    "1. Light: Light indicates if a crossing is green or red, as we want the car to cross the intersection when the signal is green, it was included as a state in the model. \n",
    "2. Next waypoint: It is crucial to know which direction to turn to, to go towards target. Therefore, I included this as a state. \n",
    "3. Oncoming, Left, Right: I also included the directions of the oncoming cars. \n",
    "\n",
    "I did not include deadline in the model. Mainly because the planner returns the shortest path, and as traffic is low (only 3 cars), the chances of the smartcab getting stuck due to traffic and having to adopt a different strategy to meet deadline is highly unlikely. Further, adding deadline would require that all the states be visited, and as deadline is a countinuous variable, it could result in significantly large number of states that have to be mapped to actions. This could have been avoided by splitting deadline into coaser qualitative intervals (such as more, normal, short) to get lower number of state combinations. However, I did not include deadline in the model because for this application, the deadline is expected to have little impact.\n",
    "\n",
    "\n",
    "#### Question: \n",
    "How many states in total exist for the smartcab in this environment? Does this number seem reasonable given that the goal of Q-Learning is to learn and make informed decisions about each state? Why or why not?\n",
    "\n",
    "#### Response: \n",
    "\n",
    "The total number of possible of states is 2 (T/F for light) X 3 (F/L/R next waypoint) X 4 (N/F/L/R for oncoming) X 4 (N/F/L/R for Left) X 4 (N/F/L/R for Right) = 384. The total number of controls possible is 4 (N/F/L/R), therefore, total number of state-control combinations possible is 1536. As we are running simulations for 1000 trials, and assuming the car moves on average 20 times per trial, it is highly likely that the car will visit most of the 1536 possible states. Further, most states are highly unlikely because there are only 3 other cars in the environment. So the states corresping to the situation that all cars are at the same intersection is highly unlikely. Therefore, it would be ok for training purposes to ignore the oncoming traffic variables and train only using 6 state variables, 2 (T/F for light) X 3 (F/L/R next waypoint) and 4 controls (N/F/L/R), i.e. 24 state-control combinations. To test this, I will compare the performance of controllers based on 2 states (light and waypoint) and on 5 (light, waypoint, and 3 oncoming) states. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement a Q-Learning Driving Agent\n",
    "\n",
    "The Q-Learning algorithm was implemented:\n",
    "1. Set the \\\\( \\gamma \\\\) and \\\\( \\alpha \\\\) parameter, and environment rewards in matrix R.\n",
    "2. Select a random initial state. If the state-action pair has not been visited previously, set \\\\( Q(state,action)=0 .\\\\)\n",
    "4. Do While the goal state hasn't been reached.\n",
    "    - Select one among all possible actions for the current state. If a state-action pair has not been visited previously, set \\\\( Q(state,action)=0 .\\\\) for them.\n",
    "    - Using this possible action, consider going to the next state.\n",
    "    - Get maximum Q value for this next state based on all possible actions.\n",
    "    - Compute: \\\\( Q(state, action) = Q(state, action)  + \\alpha \\left[ R(state, action) + \\gamma * max[Q(next~state, all~actions)] \\right] \\\\)\n",
    "    - Set the next state as the current state.\n",
    "    \n",
    "The model above has 2 parameters, \\\\( \\alpha \\\\) and \\\\( \\gamma \\\\), for the initial testing, \\\\( \\gamma \\\\) characterizes how much we trust the result from unconverged \\\\( Q \\\\), and \\\\( alpha \\\\) characterizes how much we trust the current trial to be similar to the states we expect to see in operating conditions. For initial testing,  \\\\( \\alpha = 1 \\\\) and \\\\( \\gamma = 0.1\\\\) were selected. After initial experimentation, crossvalidation was performed to obtain the best \\\\( \\alpha \\\\) and \\\\( \\gamma \\\\) values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'valid_actions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d6bda4b8c070>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# TODO: Select action according to your policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mQ_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0maction_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalid_actions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mstr_state_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mstr_state_action\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'valid_actions' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO: Select action according to your policy\n",
    "Q_actions = []\n",
    "for action_i in valid_actions:\n",
    "    str_state_action = [str(s) for s in self.state ]\n",
    "    str_state_action.append(str(action_i))\n",
    "    str_state_action  = \",\".join(str_state_action)\n",
    "    if len(self.Q_prev)==0:\n",
    "        self.Q_prev[str_state_action] = 0\n",
    "        Q_actions.append(0)\n",
    "    else:\n",
    "        if str_state_action in self.Q_prev.keys():\n",
    "            Q_actions.append(self.Q_prev[str_state_action])\n",
    "        else:\n",
    "            self.Q_prev[str_state_action] = 0\n",
    "            Q_actions.append(0)\n",
    "\n",
    "Q_max = max(Q_actions)\n",
    "action_max_inds = [valid_actions[i] for i in range(len(valid_actions)) if Q_max == Q_actions[i]]\n",
    "action = random.choice(action_max_inds)\n",
    "\n",
    "str_state_action_now = [str(s) for s in self.state ]\n",
    "str_state_action_now.append(str(action))\n",
    "str_state_action_now  = \",\".join(str_state_action_now)\n",
    "\n",
    "# Execute action and get reward\n",
    "reward = self.env.act(self, action)\n",
    "self.Q_prev[str_state_action_now] += self.alpha*(reward + self.gamma*Q_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Question: \n",
    "What changes do you notice in the agent's behavior when compared to the basic driving agent when random actions were always taken? Why is this behavior occurring?\n",
    "\n",
    "### Response:\n",
    "\n",
    "Overall, the smart cab was able to reach the target destination within deadline most of the times. The smartcab learned how to drive around in the environment. More surprisingly, the car learned this in less than 2-3 trials, and in most cases within 1 trial. I compared performance of the smartcab using Q-learning while operating in environment with 3, 20 or 40 other cars/agents. I also tested the effect of using 2 (traffic,  and waypoint) vs 5 ( traffic, oncoming and waypoint) state variables for learning. \n",
    "\n",
    "#### Random vs Q-learning with 3, 20 and 40 other agents with traffic, oncoming and waypoint state variables. \n",
    "\n",
    "Figure, below shows the difference between deadline and number of steps, a positive number indicates how early the smartcab arrived. For values below 0 (indicated by blue line), the smartcab took longer than deadline, and the cases where the smartcab did not find target are indicated by red dots. \n",
    "\n",
    "<img src=\"random_state_W2040_5states.png\">\n",
    "\n",
    "Figure above compares random agent to the outcome from Q-learning for the cases when there are 3 and 20 agents in the environment. As can be seen, for most of the cases, the Q-learning algorithm took the smartcab to the target location before the deadline, inspite of the fact that deadline was not included in the state space model. Table below presents the state-action pairs sorted by the value of Q-function. In all 3 scenarios, the smartcab moves ahead if the signal is green, waypoint is ahead and there is no oncoming traffic. Further, in all cases, the states corresponding to no oncoming traffic dominate. Probabaly because states where no traffic is present is more likely.\n",
    "\n",
    "\n",
    "\n",
    "<div>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>Q-3 (light,oncoming,left,right,way-point,action)</th>\n",
    "      <th>Q-20 (light,oncoming,left,right,way-point,action)</th>\n",
    "      <th>Q-40 (light,oncoming,left,right,way-point,action)</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>(green,None,None,None,forward,forward)</td>\n",
    "      <td>(green,None,None,None,forward,forward)</td>\n",
    "      <td>(green,None,None,None,forward,forward)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>(green,None,None,None,left,left)</td>\n",
    "      <td>(green,None,None,None,right,right)</td>\n",
    "      <td>(green,None,None,None,right,right)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>(red,None,None,None,right,right)</td>\n",
    "      <td>(green,None,None,None,left,left)</td>\n",
    "      <td>(green,None,None,None,left,left)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <td>(green,None,None,None,right,right)</td>\n",
    "      <td>(red,None,None,None,right,right)</td>\n",
    "      <td>(red,None,None,None,right,right)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>4</th>\n",
    "      <td>(green,left,None,None,forward,forward)</td>\n",
    "      <td>(green,None,None,right,forward,forward)</td>\n",
    "      <td>(green,None,None,right,forward,forward)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>5</th>\n",
    "      <td>(green,None,None,forward,forward,forward)</td>\n",
    "      <td>(green,left,None,None,forward,forward)</td>\n",
    "      <td>(green,None,left,None,forward,forward)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>6</th>\n",
    "      <td>(green,None,None,left,left,left)</td>\n",
    "      <td>(green,None,None,left,forward,forward)</td>\n",
    "      <td>(green,left,None,None,left,left)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>7</th>\n",
    "      <td>(green,None,None,right,forward,forward)</td>\n",
    "      <td>(green,None,None,forward,forward,forward)</td>\n",
    "      <td>(green,None,None,forward,forward,forward)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>8</th>\n",
    "      <td>(green,None,left,None,right,right)</td>\n",
    "      <td>(green,None,left,None,forward,forward)</td>\n",
    "      <td>(green,None,None,left,forward,forward)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>9</th>\n",
    "      <td>(green,None,None,left,forward,forward)</td>\n",
    "      <td>(green,None,forward,None,right,right)</td>\n",
    "      <td>(green,right,None,None,forward,forward)</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>\n",
    "\n",
    "\n",
    "#### Q-learning for 2 ( traffic,  and waypoint state variables) and 5 states ( traffic, oncoming and waypoint state variables ) with 3, 20 and 40 other agents. \n",
    "\n",
    "Figures below present performance of smartcab (deadline - steps) for the cases when 5 and 2 states were used. Figure on top shows that when all 5 states were available, the car was able to learn how to deal with multiple scenarios that may arise due to traffic. However, when the state variables relating to oncoming traffic were not included, the smartcab performed well when there were only 3 agents. But with more agents in the environment, the smartcab's performance degraded. This degradation happened later in training primarily because the smart cab by then had learned a bad strategy already. \n",
    "\n",
    "<img src=\"Q_5states_32040.png\">\n",
    "<img src=\"Q_2states_32040.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improve the Q-Learning Driving Agent\n",
    "Your final task for this project is to enhance your driving agent so that, after sufficient training, the smartcab is able to reach the destination within the allotted time safely and efficiently. Parameters in the Q-Learning algorithm, such as the learning rate (alpha), the discount factor (gamma) and the exploration rate (epsilon) all contribute to the driving agent’s ability to learn the best action for each state. To improve on the success of your smartcab:\n",
    "\n",
    "Set the number of trials, n_trials, in the simulation to 100.\n",
    "Run the simulation with the deadline enforcement enforce_deadline set to True (you will need to reduce the update delay update_delay and set the display to False).\n",
    "Observe the driving agent’s learning and smartcab’s success rate, particularly during the later trials.\n",
    "Adjust one or several of the above parameters and iterate this process.\n",
    "This task is complete once you have arrived at what you determine is the best combination of parameters required for your driving agent to learn successfully.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: \n",
    "Report the different values for the parameters tuned in your basic implementation of Q-Learning. For which set of parameters does the agent perform best? How well does the final driving agent perform?\n",
    "\n",
    "\n",
    "### Question: \n",
    "Does your agent get close to finding an optimal policy, i.e. reach the destination in the minimum possible time, and not incur any penalties? How would you describe an optimal policy for this problem?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
